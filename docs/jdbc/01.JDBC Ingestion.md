# RDB Data Ingestion via JDBC

### **Features**

RDB incremental data into the hoodie. RDB here represents most common relational databases (such as mysql, Oracle MSSQL) or data sources with JDBC protocol, such as hive.

【TIPS】 During data collection, you need to specify the jdbc driver dependency of the collected RDB data source through the --jar parameter. 


### **Configuration**

| Configuration | Description                                                  | Level             |
| ------------- | ------------------------------------------------------------ | ----------------- |
| --dialect     | Supprot build-in RDB (mysql,postgresql,db2,sqlServer,oracle,teradata,h2). | required          |
| --table       | The database table to read data from.                        | required          |
| --debug       | If you set debug mode, you can see a small amount of data in terminal. | optional（false） |


### *How to configure properties？*

The configuration is divided into the following three parts：

（1）**Read JDBC extra optional parameter** 

For the attachment parameters of mongodb, please refer to ：[JDBC configuration](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala).

The configuration method in the resource file is '**hoodie.deltastreamer.jdbc.extra.options.**' splicing with official native parameters.

Some important configurations are as follows：

| Configuration   | Description                                                  | Level    |
| --------------- | ------------------------------------------------------------ | -------- |
| url             | Database connection address URL. e.g. MySQL is jdbc:mysql://ip:port/dbname?... | require  |
| driver          | Database driven class name. e.g. MySQL is com.mysql.jdbc.Driver. | require  |
| user            | Database access username.                                    | require  |
| password        | Database access password.                                    | require  |
| dbtable         | Database table name.                                         | require  |
| query           | Table name or a table subquery.  <br />Both query and dbtable just be specified at the same time.                            | optional  |
| partitionColumn | The column used to partition. <br />Must be a number, date, or timestamp column in a related table. | optional |
| lowerBound      | The lower bound of partition column                          | optional |
| upperBound      | The upper bound of the partition column                      | optional |
| numPartitions   | The number of partitions. <br />The maximum number of partitions a table read can use for parallelism. This also determines the maximum number of concurrent JDBC connections | optional |
| queryTimeout | Tthe number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit, default: 0. | optional |
| fetchsize | Pull the number of strips. <br />It determines how many rows to get for each round trip, which helps the performance of the jdbc driver. The jdbc driver defaults to a lower get size. | optional |
| sessionInitStatement | An option to execute custom SQL before fetching data from the remote DB. <br />Execute the SQL statement (or pl/ SQL block) customized by this option after opening each database session to the remote database and before starting reading data. Use it to implement the session initialization code. <br />Example: <br />option("sessionInitStatement", """BEGIN execute immediate 'alter session set "_serial_direct_read"=true'; END;""") | optional |
| customSchema | Custom data structure.<br />Used to customize the structure when reading data from the JDBC connector. For example: id DECIMAL(38,0),name STRING<br />You can also specify some fields, and other fields use the default type mapping.<br />The column name should be the same as the corresponding column name of the JDBC table. The user can specify the corresponding data type of sparksql without using the default value | optional |
| pushDownPredicate | An option to allow/disallow pushing down predicate into JDBC data source, default: true<br />In this case, spark will push down the filter to the JDBC data source as much as possible.<br/><br/>Otherwise, if set to false, no filter will be pushed down to the JDBC data source, so all filters will be processed by spark.<br/><br/>When spark performs predicate filtering faster than JDBC data sources, predicate pushdown is usually turned off. | optional |

（2）**Hoodie optional parameter**

The parameters imported into Hoodie are consistent with the official native parameters. Please refer to the official website.

（3）**transform for data ingestion**

Before the data is imported into Hoodie, simple ETL processing can be carried out through parameters of '**hoodie.deltastreamer.transformer.sql**'. 

Here you can fill in a simple SQL syntax to preprocess the dataset.

### *FAQ*

TODO

# Document Data Ingestion via MongoDB

### **Features**

MongoDB incremental data into the hoodie. The environment is as follows：

（1）database

- MongoDB：4.0

（2）library

- mongo-spark-connector：3.0.1 （require）

- mongo-java-driver：3.12.10 （require）

【TIPS】 The above library need to be specified through the --jar parameter when submitting the spark application.


### **Configuration**

| Configuration | Description                                                  | Level             |
| ------------- | ------------------------------------------------------------ | ----------------- |
| --uri         | The connection string in the form mongodb://host:port/.      | required          |
| --database    | The database name to read data from.                         | required          |
| --collection  | The collection name to read data from.                       | required          |
| --props       | Path to properties file on localfs or dfs.                   | required          |
| --debug       | If you set debug mode, you can see a small amount of data in terminal. | optional（false） |


### *How to configure properties？*

The configuration is divided into the following three parts：

（1）**Read MongoDB extra optional parameter** 

For the attachment parameters of mongodb, please refer to ：[mongodb spark-connector configuration](https://www.mongodb.com/docs/spark-connector/master/configuration/).

The configuration method in the resource file is '**hoodie.deltastreamer.mongodb.extra.options.**' splicing with official native parameters.

（2）**Hoodie optional parameter**

The parameters imported into Hoodie are consistent with the official native parameters. Please refer to the official website.

（3）**transform for data ingestion**

Before the data is imported into Hoodie, simple ETL processing can be carried out through parameters of '**hoodie.deltastreamer.transformer.sql**'. 

Here you can fill in a simple SQL syntax to preprocess the dataset.

### *FAQ*

（1）How to flatten nested data type？

When you set hoodie.deltastreamer.es.auto.flatten.enable is true，the embedded data type is automatically flattened to the leaf node. The main types are Array<Struct<>> and Struct<>. Refer to the following example:

```json
{
  "id": 1,
  "locs": [
    {
      "loc_id": "Axyxuii=",
      "x": 12.65,
      "y": 74.76
    },
    {
      "loc_id": "WWsexo=",
      "x": 32.15,
      "y": 18.19
    }
  ]
}
```

After flattening, the data is：

| id   | locs_loc_id | locs_x | locs_y |
| ---- | ----------- | ------ | ------ |
| 1    | Axyxuii=    | 12.65  | 74.76  |
| 1    | WWsexo=     | 32.15  | 18.19  |

It should be noted that，If Auto flattening is not enabled, it should be a piece of data，At this time, it is assumed that the set **hoodie key** to id. After automatic flattening is turned on, the data is theoretically two record. At this time, if the Hoodie key is not readjusted, the data may be lost! For example, in this example, we set hoodie key to id and locs_loc_id. **In other words, once the auto flattening function is turned on, it needs to be reconsidered hoodie key.**

Let's take another example,

```bson
{
    "_id" : ObjectId("6241579fa50aeed0adfbe58e"),
    "id" : 1.0,
    "locs" : [ 
        {
            "loc_id" : "Axyxuii=",
            "x" : 12.65,
            "y" : 74.76
        }, 
        {
            "loc_id" : "WWsexo=",
            "x" : 32.15,
            "y" : 18.19
        }
    ],
    "arr" : [ 
        {
            "id" : "Axyxuii=",
            "a" : 12.65,
            "b" : 74.76
        }, 
        {
            "id" : "WWsexo=",
            "a" : 32.15,
            "b" : 18.19
        }, 
        {
            "id" : "WWsexo=",
            "a" : 32.15,
            "b" : 18.19
        }
    ]
}
```

After flattening, the data is：

| id   | _id_oid                  | arr_id   | arr_a | arr_b | locs_loc_id | locs_x | locs_y |
| ---- | ------------------------ | -------- | ----- | ----- | ----------- | ------ | ------ |
| 1.0  | 6241579fa50aeed0adfbe58e | Axyxuii= | 12.65 | 74.76 | Axyxuii=    | 12.65  | 74.76  |
| 1.0  | 6241579fa50aeed0adfbe58e | Axyxuii= | 12.65 | 74.76 | WWsexo=     | 32.15  | 18.19  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | Axyxuii=    | 12.65  | 74.76  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | WWsexo=     | 32.15  | 18.19  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | Axyxuii=    | 12.65  | 74.76  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | WWsexo=     | 32.15  | 18.19  |

This result may be correct or wrong, so whether automatic flattening can achieve the desired effect depends on the complexity of document metadata.

If the automatic flattening cannot meet your expectations, you can set the parameter '**hoodie.deltastreamer.transformer.sql**' customize deployment strategy.
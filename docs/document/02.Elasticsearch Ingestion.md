# Document Data Ingestion via Elasticsearch

### **Features**

Elasticsearch incremental data into the hoodie. The environment is as follows：

（1）database

- Elasticsearch：7.3 

（2）library

- elasticsearch-spark-30_：7.12 （require）

- hadoop：2.10.x

【TIPS】 

（1）The library of elasticsearch-spark-30_ need to be specified through the --jar parameter when submitting the spark application.

（2）We recommend to use this hadoop version for maven dependency of 2.10.x. Exceptions may occur below this version.

### **Configuration**

| Configuration | Description                                                  | Level             |
| ------------- | ------------------------------------------------------------ | ----------------- |
| --resource    | Elasticsearch resource location, where data is read and written to. | required          |
| --nodes       | List of Elasticsearch nodes to connect to.                   | required          |
| --port        | Default HTTP/REST port used for connecting to Elasticsearch - this setting is applied to the nodes in es. | required          |
| --props       | Path to properties file on localfs or dfs.                   | required          |
| --debug       | If you set debug mode, you can see a small amount of data in terminal. | optional（false） |

### *How to configure properties？*

The configuration is divided into the following three parts：

（1）**Read Elasticsearch extra optional parameter**

For the attachment parameters of es, please refer to ：[elasticsearch hadoop configuration](https://www.elastic.co/guide/en/elasticsearch/hadoop/7.3/configuration.html#configuration).

The configuration method in the resource file is '**hoodie.deltastreamer.es.extra.options.**' splicing with official native parameters.

（2）**Hoodie optional parameter**

The parameters imported into Hoodie are consistent with the official native parameters. Please refer to the official website.

（3）**transform for data ingestion**

Before the data is imported into Hoodie, simple ETL processing can be carried out through parameters of '**hoodie.deltastreamer.transformer.sql**'.

Here you can fill in a simple SQL syntax to preprocess the dataset.

### *FAQ*

（1）How to flatten nested data type？

You can refer to mongodb about flattening nested data type. If the automatic flattening cannot meet your expectations, you can set the parameter '**hoodie.deltastreamer.transformer.sql**' customize deployment strategy.

（2）Exception caused by data type od Array<>

The exception stack is as follows：

```tex
[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] WARN  org.elasticsearch.spark.sql.ScalaRowValueReader  - Field 'space.disabledFeatures' is backed by an array but the associated Spark Schema does not reflect this;
              (use es.read.field.as.array.include/exclude) 
[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] WARN  org.elasticsearch.spark.sql.ScalaRowValueReader  - Field 'space.disabledFeaturesArray' is backed by an array but the associated Spark Schema does not reflect this;
              (use es.read.field.as.array.include/exclude) 
[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)
org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Field 'space.disabledFeaturesArray.y' not found; typically this occurs with arrays which are not mapped as single value
```

Field **space.disabledFeaturesArray** is an array type in es. All array types need to be specified through the '**es.read.field.as.array.include**' parameter. In this example, you need to add the following configuration：

```properties
es.read.field.as.array.include=space.disabledFeaturesArray
```

If more than one is separated by commas.

（3）Exception caused by duplicate column

The exception stack is as follows：

```tex
Exception in thread "main" org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema: `outflag`
```

This problem may be caused by Spark's insensitivity to column case，Mapping in ES is as follows：

```json
{
    "es_tp": {
        "mappings": {
            "properties": {
                "batchId": {
                    "type": "keyword"
                },
                ...
                "outFlag": {
                    "type": "keyword"
                },
                "outflag": {
                    "type": "long"
                },
                ...
            }
        }
    }
}
```

In es, outFlag and outflag are two different columns. At this time, you can avoid this problem by removing unused columns with parameter '**es.read.field.exclude**'.

```properties
es.read.field.exclude=outflag
```

（4）Time zone problem of extracting data

If there is a time zone problem when extracting data, you can turn off parameter 'es.mapping.date.rich', default value is true.